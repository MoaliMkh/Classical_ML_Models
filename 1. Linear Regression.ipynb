{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16adc323",
   "metadata": {},
   "source": [
    "<br><font face=\"Times New Roman\" size=5><div dir=ltr align=center>\n",
    "<font color=blue size=8>\n",
    "    Introduction to Machine Learning <br>\n",
    "<font color=red size=5>\n",
    "    Sharif University of Technology - Computer Engineering Department <br>\n",
    "    Fall 2022<br> <br>\n",
    "<font color=black size=6>\n",
    "    Homework 2: Practical - Linear Regression\n",
    "    </div>\n",
    "<br><br>\n",
    "<font size=4>\n",
    "   **Name**: **MohammadAli MohammadKhani**<br>\n",
    "   **Student ID**: **98102251**<br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585264a",
   "metadata": {},
   "source": [
    "<font face=\"Times New Roman\" size=4><div dir=ltr>\n",
    "# Problem 1: Linear Regression Model (40 + 30 optional points)\n",
    "According to <a href=\"https://github.com/asharifiz/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_02_Classical_Models/Linear%20regression.ipynb\"><font face=\"Roboto\">Linear Regression Notebook</font></a>, train a linear regression model on an arbitrary dataset. Explain your chosen dataset and split your data into train and test sets, then predict values for the test set using your trained model. Try to find the best hyperparameters for your model. (Using Lasso Regression, Ridge Regression or Elastic Net and comparing them will have extra optional points)\n",
    "<br> Explain each step of your workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387d3a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "diabetes_dataset = datasets.load_diabetes(return_X_y=True, as_frame=True)\n",
    "\n",
    "X, Y = diabetes_dataset[0], diabetes_dataset[1]      ## Loading Features (X) and targets (Y)\n",
    "\n",
    "train_X = X.iloc[:200, :].to_numpy()\n",
    "\n",
    "\n",
    "train_Y = Y.iloc[:200].to_numpy()\n",
    "test_X = X.iloc[420: 442, :].to_numpy()\n",
    "test_Y = Y.iloc[420: 442].to_numpy()      ### Splitting dataset into train and test in a proportion of about 80%\n",
    "\n",
    "train_X = np.append(np.ones((200, 1)), train_X , axis=1)   ### Adding 1 to each row to include multiplying b \n",
    "test_X = np.append(np.ones((22, 1)), test_X , axis=1)   ### Adding 1 to each row to include multiplying b \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10055aa",
   "metadata": {},
   "source": [
    "### DataSet Explanantion\n",
    "This dataset has 10 featured, named below:\n",
    "##### Age\n",
    "##### Sex\n",
    "##### BMI\n",
    "##### BP\n",
    "##### S1\n",
    "##### S2\n",
    "##### S3\n",
    "##### S4\n",
    "##### S5\n",
    "##### S6\n",
    "\n",
    "All of these features have numerical values, so we won't worry about nan values, also, they are related to physical and health-related issues, so we don't need to understand them all. This dataset consists of 2 parts, included in a tuple. The first part includes features and the second part is targets, which are consistent values, so we can pergorm regression on them. We train and test our model in 3 ways:\n",
    "##### Dot Product and updating weights' vector using gradient descent: Doing this iterationally, we constantly make predictions using dot product and then we update our weights vector.\n",
    "##### Closed Form: Using a closed form introduced in the next parts, we can simply obtain a finalized weights vector.\n",
    "##### Using sklearn: In this scenario, we just import the needed functions from sklearn and call the fit() and predict() functions and get the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a970e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(11, 1) ## Initializing random weights, one element more because of including bias.\n",
    "\n",
    "def closed_solution(X, Y):\n",
    "    W = np.dot((np.linalg.inv(np.dot(X.T,X))), np.dot(X.T,Y))  ## Find the weights using the formula  W = (X ^ (T) * X) ^ (-1) * X^(T)*Y\n",
    "    return W\n",
    "\n",
    "def make_predictions(X, W):\n",
    "    prediction = np.dot(X, W)    #### Dot product to make predictions\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def calculate_loss(pred, targ):\n",
    "    residual = pred - targ\n",
    "    return np.sum(residual * residual) / residual.size  ### Calculate MSE loss\n",
    "    \n",
    "def update_weights(old_weights, learning_rate, gradient):\n",
    "    new_weights = old_weights - learning_rate * gradient\n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f7342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_lr_gd(X, Y, W, n_epoches, lr):\n",
    "    losses = []\n",
    "    for i in range(n_epoches):\n",
    "        predictions = make_predictions(X, W) ## Making Predictions\n",
    "        error = predictions - Y # calculating the loss\n",
    "        loss = calculate_loss(predictions, Y) # adding the loss to our loss list \n",
    "        losses.append(loss) # calculating gradients\n",
    "        gradient = np.sum(error) / Y.size # updating weights and biases\n",
    "        W = update_weights(W, lr, gradient)\n",
    "        \n",
    "    return W, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9deef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The starting loss is: 26853.760508514988\n",
      "The Final loss is: 7058.199118602678\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.009\n",
    "n_epoches = 200\n",
    "W, first_method_losses = train_model_lr_gd(train_X, train_Y, W, n_epoches, learning_rate)\n",
    "print(\"The starting loss is: \" + str(first_method_losses[0]))\n",
    "print(\"The Final loss is: \" + str(first_method_losses[len(first_method_losses) - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e2fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6785.660254752159\n"
     ]
    }
   ],
   "source": [
    "### Now We want to test the model\n",
    "\n",
    "predictions = make_predictions(X=test_X, W=W)\n",
    "print(calculate_loss(predictions, test_Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e12b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for train is: 2698.1591490916035\n",
      "Loss for test is: 1276.4517603050972\n"
     ]
    }
   ],
   "source": [
    "### Now lets make predictions using the closed form this time:\n",
    "\n",
    "weights = closed_solution(train_X, train_Y)\n",
    "predictions = make_predictions(train_X, weights)\n",
    "print(\"Loss for train is: \" + str(calculate_loss(predictions, train_Y)))\n",
    "\n",
    "weights = closed_solution(test_X, test_Y)\n",
    "predictions = make_predictions(test_X, weights)\n",
    "print(\"Loss for test is: \" + str(calculate_loss(predictions, test_Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e3e7da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5133662718598058\n"
     ]
    }
   ],
   "source": [
    "## Now lets do it by using sklearn perse:\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(train_X, train_Y)\n",
    "score = model.score(train_X, train_Y)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c4832c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1764.9294214745664\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_X)\n",
    "sklearn_loss = calculate_loss(predictions, test_Y)\n",
    "print(sklearn_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988a94de",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "comparing these 3 methods, as we can see, the loss for the third method (using sklearn) is the least. Then there is the closed form loss, and above all, the dot product method has the highest loss. So, the sklearn method is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b095c1",
   "metadata": {},
   "source": [
    "### Now We want to use Lasso and ElasticNet and Ridge, to observe what changes thet bring to out regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3b9e682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso loss is: 2192.4702930938747\n",
      "ElasticNet loss is: 4868.80839043385\n",
      "Ridge loss is: 2432.360867217826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, ElasticNet, Ridge\n",
    "\n",
    "lasso_regressior = Lasso(alpha=0.5)\n",
    "lasso_regressior.fit(train_X, train_Y)\n",
    "predictions = lasso_regressior.predict(test_X)\n",
    "Lasso_loss = calculate_loss(predictions, test_Y)\n",
    "print(\"Lasso loss is: \" + str(Lasso_loss))\n",
    "\n",
    "\n",
    "ElasticNet_regressior = ElasticNet(alpha=0.5)\n",
    "ElasticNet_regressior.fit(train_X, train_Y)\n",
    "predictions = ElasticNet_regressior.predict(test_X)\n",
    "ElasticNet_loss = calculate_loss(predictions, test_Y)\n",
    "print(\"ElasticNet loss is: \" + str(ElasticNet_loss))\n",
    "\n",
    "\n",
    "\n",
    "ridge_regressior = Ridge(alpha=0.5)\n",
    "ridge_regressior.fit(train_X, train_Y)\n",
    "predictions = ridge_regressior.predict(test_X)\n",
    "ridge_loss = calculate_loss(predictions, test_Y)\n",
    "print(\"Ridge loss is: \" + str(ridge_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdd3ae4",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "As we can see, The loss for Ridge is the lowest. Then Lasso and after that, ElasticNet has a higher loss. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
